{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete implementation of class kNN for tasks B, C, and D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class of k-Nearest Neigbor Classifier\n",
    "\n",
    "\n",
    "class kNN():\n",
    "    \n",
    "     \n",
    "    def __init__(self, k = 3, exp = 2):\n",
    "    # constructor for kNN classifier \n",
    "    # k is the number of neighbor for local class estimation\n",
    "    # exp is the exponent for the Minkowski distance\n",
    "        self.k = k \n",
    "        self.exp = exp\n",
    "      \n",
    "        \n",
    "    def normalize(self, df):\n",
    "    # normalizes data fram df w.r.t X_Train\n",
    "    # use the method first to normalize X_test and then X_train.\n",
    "        column_maxes = self.X_train.max()\n",
    "        for col in df.columns:\n",
    "            df.loc[:,col]=df.loc[:,col]/column_maxes[col]\n",
    "        return df\n",
    "    \n",
    " \n",
    "    \n",
    "    def fit(self, X_train, Y_train):\n",
    "    # training k-NN method\n",
    "    # X_train is the training data given with input attributes. n-th row correponds to n-th instance.\n",
    "    # Y_train is the output data (output vector): n-th element of Y_train is the output value for n-th instance in X_train.\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train   \n",
    "         \n",
    "    def getDiscreteClassification(self, X_test):\n",
    "    # predict-class k-NN method\n",
    "    # X_test is the test data given with input attributes. Rows correpond to instances\n",
    "    # Method outputs prediction vector Y_pred_test:  n-th element of Y_pred_test is the prediction for n-th instance in X_test\n",
    "    \n",
    "   \n",
    "        Y_pred_test = [] #prediction vector Y_pred_test for all the test instances in X_test is initialized to empty list []\n",
    "\n",
    "     \n",
    "        for i in range(len(X_test)):   #iterate over all instances in X_test\n",
    "            test_instance = X_test.iloc[i] #i-th test instance \n",
    "            \n",
    "            distances = []  #list of distances of the i-th test_instance for all the train_instance s in X_train, initially empty.\n",
    "          \n",
    "            for j in range(len(self.X_train)):  #iterate over all instances in X_train\n",
    "                train_instance = self.X_train.iloc[j] #j-th training instance \n",
    "                distance = self.Minkowski_distance(test_instance, train_instance) #distance between i-th test instance and j-th training instance  \n",
    "                distances.append(distance) #add the distance to the list of distances of the i-th test_instance\n",
    "        \n",
    "            # Store distances in a dataframe. The dataframe has the index of Y_train in order to keep the correspondence with the classes of the training instances \n",
    "            df_dists = pd.DataFrame(data=distances, columns=['dist'], index = self.Y_train.index)\n",
    "        \n",
    "            # Sort distances, and only consider the k closest points in the new dataframe df_knn\n",
    "            df_nn = df_dists.sort_values(by=['dist'], axis=0)\n",
    "            df_knn =  df_nn[:self.k]\n",
    "            \n",
    "            # Note that the index df_knn.index of df_knn contains indices in Y_train of the k-closed training instances to \n",
    "            # the i-th test instance. Thus, the dataframe self.Y_train[df_knn.index] contains the classes of those k-closed \n",
    "            # training instances. Method value_counts() computes the counts (number of occurencies) for each class in \n",
    "            # self.Y_train[df_knn.index] in dataframe predictions. \n",
    "            predictions = self.Y_train[df_knn.index].value_counts()\n",
    "                 \n",
    "            # the first element of the index predictions.index contains the class with the highest count; i.e. the prediction y_pred_test.\n",
    "            y_pred_test = predictions.index[0]\n",
    "\n",
    "            # add the prediction y_pred_test to the prediction vector Y_pred_test for all the test instances in X_test\n",
    "            Y_pred_test.append(y_pred_test)\n",
    "        \n",
    "        return Y_pred_test\n",
    "\n",
    "    \n",
    "    \n",
    "    def getClassProbs(self, X_test):\n",
    "    # estimates posterior class probs for k-NN \n",
    "    # X_test is the test data given with input attributes. Rows correpond to instances\n",
    "    # Method outputs prediction vector  Class_probs_all_tests: prediction vector of class probs for all the test instances in X_test\n",
    "    \n",
    "        \n",
    "        Class_probs_all_tests = [] #prediction vector of class probs for all the test instances in X_test is initialized to empty list []\n",
    "\n",
    "     \n",
    "        for i in range(len(X_test)):   #iterate over all instances in X_test\n",
    "            test_instance = X_test.iloc[i] #i-th test instance \n",
    "            \n",
    "            distances = []  #list of distances of the i-th test_instance for all the train_instance s in X_train, initially empty.\n",
    "          \n",
    "            for j in range(len(self.X_train)):  #iterate over all instances in X_train\n",
    "                train_instance = self.X_train.iloc[j] #j-th training instance \n",
    "                distance = self.Minkowski_distance(test_instance, train_instance) #distance between i-th test instance and j-th training instance  \n",
    "                distances.append(distance) #add the distance to the list of distances of the i-th test_instance\n",
    "        \n",
    "            # Store distances in a dataframe. The dataframe has the index of Y_train in order to keep the correspondence with the classes of the training instances \n",
    "            df_dists = pd.DataFrame(data=distances, columns=['dist'], index = self.Y_train.index)\n",
    "        \n",
    "            # Sort distances, and only consider the k closest points in the new dataframe df_knn\n",
    "            df_nn = df_dists.sort_values(by=['dist'], axis=0)\n",
    "            df_knn =  df_nn[:self.k]\n",
    "            \n",
    "            # Note that the index df_knn.index of df_knn contains indices in Y_train of the k-closed training instances to \n",
    "            # the i-th test instance. Thus, the dataframe self.Y_train[df_knn.index] contains the classes of those k-closed \n",
    "            # training instances. Method value_counts() computes the counts (number of occurencies) for each class in \n",
    "            # self.Y_train[df_knn.index] in dataframe predictions. \n",
    "            predictions = self.Y_train[df_knn.index].value_counts()\n",
    "             \n",
    "            class_probs = predictions / predictions.sum()\n",
    "                \n",
    "            # add class_probs of the current test instance to the prediction vector Class_probs_all_tests for all the test instances in X_test\n",
    "            Class_probs_all_tests.append(class_probs)\n",
    "        \n",
    "        return  Class_probs_all_tests\n",
    "\n",
    "    \n",
    "    \n",
    "    def getPrediction(self, X_test):\n",
    "    # estimates regression values for test instances in X_test according to the k-NN rule\n",
    "    # X_test is the test data given with input attributes. Rows correpond to instances\n",
    "    # Method outputs prediction vector Regression_values: prediction vector of regression values for all the test instances in X_test\n",
    "    \n",
    "        \n",
    "        Regression_values = [] #prediction vector of regression values for all the test instances in X_test is initialized to empty list []\n",
    "\n",
    "     \n",
    "        for i in range(len(X_test)):   #iterate over all instances in X_test\n",
    "            test_instance = X_test.iloc[i] #i-th test instance \n",
    "            \n",
    "            distances = []  #list of distances of the i-th test_instance for all the train_instance s in X_train, initially empty.\n",
    "          \n",
    "            for j in range(len(self.X_train)):  #iterate over all instances in X_train\n",
    "                train_instance = self.X_train.iloc[j] #j-th training instance \n",
    "                distance = self.Minkowski_distance(test_instance, train_instance) #distance between i-th test instance and j-th training instance  \n",
    "                distances.append(distance) #add the distance to the list of distances of the i-th test_instance\n",
    "        \n",
    "            # Store distances in a dataframe. The dataframe has the index of Y_train in order to keep the correspondence with the classes of the training instances \n",
    "            df_dists = pd.DataFrame(data=distances, columns=['dist'], index = self.Y_train.index)\n",
    "        \n",
    "            # Sort distances, and only consider the k closest points in the new dataframe df_knn\n",
    "            df_nn = df_dists.sort_values(by=['dist'], axis=0)\n",
    "            df_knn =  df_nn[:self.k]\n",
    "            \n",
    "            # Note that the index df_knn.index of df_knn contains indices in Y_train of the k-closed training instances to \n",
    "            # the i-th test instance. Thus, the dataframe self.Y_train[df_knn.index] contains the regerssion values of those k-closed \n",
    "            # training instances.\n",
    "            \n",
    "            predictions = self.Y_train[df_knn.index] \n",
    "             \n",
    "            predicted_regression_value = predictions.sum() / len(predictions)\n",
    "                \n",
    "            # add regression value of the current test instance to the prediction vector  Regression_values for all the test instances in X_test\n",
    "            Regression_values.append(predicted_regression_value)\n",
    "        \n",
    "        return  Regression_values\n",
    "\n",
    "    \n",
    "    \n",
    "    def Minkowski_distance(self, x1, x2):\n",
    "    # computes the Minkowski distance of x1 and x2 for two labeled instances (x1,y1) and (x2,y2)\n",
    "    \n",
    "        # Set initial distance to 0\n",
    "        distance = 0\n",
    "    \n",
    "        # Calculate Minkowski distance using the exponent exp\n",
    "        for i in range(len(x1)):\n",
    "            distance = distance + abs(x1[i] - x2[i])**self.exp\n",
    "        \n",
    "        distance = distance**(1/self.exp)\n",
    "    \n",
    "        return distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests on the glass data without normalization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from numpy.random import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "##################################################\n",
    "# No Normalization Experiments\n",
    "##################################################\n",
    "\n",
    "##################################################\n",
    "# Hold-out testing: Training and Test set creation\n",
    "##################################################\n",
    "\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "data.head()\n",
    "Y = data['class']\n",
    "X = data.drop(['class'],axis=1)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.34, random_state=10)\n",
    "\n",
    "\n",
    "# range for the values of parameter k for kNN\n",
    "\n",
    "#k_range = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31]\n",
    "\n",
    "k_range = [1, 3, 5, 7, 9, 11]\n",
    "\n",
    "trainAcc = np.zeros(len(k_range))\n",
    "testAcc = np.zeros(len(k_range))\n",
    "\n",
    "\n",
    "index = 0 \n",
    "for k  in  k_range:\n",
    "    clf = kNN(k)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    Y_predTrain = clf.getDiscreteClassification(X_train)\n",
    "    Y_predTest = clf.getDiscreteClassification(X_test)\n",
    "    trainAcc[index] = accuracy_score(Y_train, Y_predTrain)\n",
    "    testAcc[index] = accuracy_score(Y_test, Y_predTest)\n",
    "    index += 1\n",
    "   \n",
    "    \n",
    "#########################################\n",
    "# Plot of training and test accuracies\n",
    "#########################################\n",
    "    \n",
    "plt.plot(k_range,trainAcc,'ro-',k_range,testAcc,'bv--')\n",
    "plt.legend(['Training Accuracy','Test Accuracy'])\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Accuracy: No normalization')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests on the glass data with normalization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from numpy.random import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "##################################################\n",
    "# Experiments with Normalization\n",
    "##################################################\n",
    "\n",
    "##################################################\n",
    "# Hold-out testing: Training and Test set creation\n",
    "##################################################\n",
    "\n",
    "data = pd.read_csv('glass.csv')\n",
    "data.head()\n",
    "Y = data['class']\n",
    "X = data.drop(['class'],axis=1)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.34, random_state=10)\n",
    "\n",
    "\n",
    "# range for the values of parameter k for kNN\n",
    "\n",
    "#k_range = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31]\n",
    "\n",
    "k_range = [1, 3, 5, 7, 9, 11]\n",
    "\n",
    "trainAcc = np.zeros(len(k_range))\n",
    "testAcc = np.zeros(len(k_range))\n",
    "\n",
    "\n",
    "index = 0 \n",
    "for k  in  k_range:\n",
    "    clf = kNN(k)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    X_test = clf.normalize(X_test)\n",
    "    X_train = clf.normalize(X_train)\n",
    "    Y_predTrain = clf.getDiscreteClassification(X_train)\n",
    "    Y_predTest = clf.getDiscreteClassification(X_test)\n",
    "    trainAcc[index] = accuracy_score(Y_train, Y_predTrain)\n",
    "    testAcc[index] = accuracy_score(Y_test, Y_predTest)\n",
    "    index += 1\n",
    "   \n",
    "    \n",
    "#########################################\n",
    "# Plot of training and test accuracies\n",
    "#########################################\n",
    "    \n",
    "plt.plot(k_range,trainAcc,'ro-',k_range,testAcc,'bv--')\n",
    "plt.legend(['Training Accuracy','Test Accuracy'])\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Accuracy: Normalization')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Answer 1: Normalization does change the results. For the glass data on the current hold-out split, however, it does not improve the test accuracy. Moreover, the kNN classifiers overfits for a longer range of k. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# range for the values of parameter exp for kNN\n",
    "\n",
    "exp_range = [2,  100, 10000]\n",
    "\n",
    "trainAcc = np.zeros(len(exp_range))\n",
    "testAcc = np.zeros(len(exp_range))\n",
    "\n",
    "\n",
    "index = 0 \n",
    "for exp  in  exp_range:\n",
    "    clf = kNN(k = 3, exp = exp)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    Y_predTrain = clf.getDiscreteClassification(X_train)\n",
    "    Y_predTest = clf.getDiscreteClassification(X_test)\n",
    "    trainAcc[index] = accuracy_score(Y_train, Y_predTrain)\n",
    "    testAcc[index] = accuracy_score(Y_test, Y_predTest)\n",
    "    index += 1\n",
    "   \n",
    "    \n",
    "#########################################\n",
    "# Plot of training and test accuracies\n",
    "#########################################\n",
    "    \n",
    "plt.plot(exp_range,trainAcc,'ro-',exp_range,testAcc,'bv--')\n",
    "plt.legend(['Training Accuracy','Test Accuracy'])\n",
    "plt.xlabel('exp')\n",
    "plt.ylabel('Accuracy: Minkowski exponent dependency')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Answer 2:  The training and hold-out accuracy rates change  with the exponent because it changes the distances between the instances. Note that the exponent can cause  feature selection in a unsupervised manner. The higher is the value of the exponent, the more features are removed. Since this happens without the class information, the accuracy rates can go in any direction.\n",
    "\n",
    "\n",
    "### C. Test code for the getClassProbs method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from numpy.random import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "##################################################\n",
    "# Hold-out testing: Training and Test set creation\n",
    "##################################################\n",
    "\n",
    "data = pd.read_csv('glass.csv')\n",
    "data.head()\n",
    "Y = data['class']\n",
    "X = data.drop(['class'],axis=1)\n",
    "\n",
    "clf = kNN(k=3)\n",
    "clf.fit(X,Y)\n",
    "Probs = clf.getClassProbs(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Test code for the getPrediction method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "##################################################\n",
    "# Hold-out testing: Training and Test set creation\n",
    "##################################################\n",
    "\n",
    "data = pd.read_csv('autoprice.csv')\n",
    "data.head()\n",
    "Y = data['class']\n",
    "X = data.drop(['class'],axis=1)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.34, random_state=10)\n",
    "\n",
    "\n",
    "# range for the values of parameter k for kNN\n",
    "\n",
    "#k_range = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31]\n",
    "\n",
    "k_range = [1, 3, 5, 7, 9, 11]\n",
    "\n",
    "trainErrors = np.zeros(len(k_range))\n",
    "testErrors = np.zeros(len(k_range))\n",
    "\n",
    "\n",
    "index = 0 \n",
    "for k  in  k_range:\n",
    "    clf = kNN(k)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    X_test = clf.normalize(X_test)\n",
    "    X_train = clf.normalize(X_train)\n",
    "    Y_predTrain = clf.getPrediction(X_train)\n",
    "    Y_predTest = clf.getPrediction(X_test)\n",
    "    trainErrors[index] = mean_absolute_error(Y_train, Y_predTrain)\n",
    "    testErrors[index] = mean_absolute_error(Y_test, Y_predTest)\n",
    "    index += 1\n",
    "   \n",
    "    \n",
    "#########################################\n",
    "# Plot of training and test accuracies\n",
    "#########################################\n",
    "    \n",
    "plt.plot(k_range,trainErrors,'ro-',k_range,testErrors,'bv--')\n",
    "plt.legend(['Training MAE Erros','Test MAE Erros'])\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('MAE Errors')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
